{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title: Blog Post 4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog post, I'll write a tutorial on a simple version of the *spectral clustering* algorithm for clustering data points. \n",
    "\n",
    "This blog post was modified from a [Jupyter Notebook](https://nbviewer.org/github/PhilChodrow/PIC16B/blob/master/HW/spectral-clustering.ipynb) written by Dr. Phil Chodrow.  \n",
    "***Note***: your blog post doesn't have to contain a lot of math. It's ok for you to give explanations like \"this function is an approximation of this other function according to the math in the written assignment.\" \n",
    "\n",
    "### Notation\n",
    "\n",
    "In all the math below: \n",
    "\n",
    "- Boldface capital letters like $\\mathbf{A}$ refer to matrices (2d arrays of numbers). \n",
    "- Boldface lowercase letters like $\\mathbf{v}$ refer to vectors (1d arrays of numbers). \n",
    "- $\\mathbf{A}\\mathbf{B}$ refers to a matrix-matrix product (`A@B`). $\\mathbf{A}\\mathbf{v}$ refers to a matrix-vector product (`A@v`). \n",
    "\n",
    "### Comments and Docstrings\n",
    "\n",
    "You should plan to comment all of your code. Docstrings are not required except in Part G. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this problem, we'll study *spectral clustering*. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure. It is a technique with roots in graph theory, where the approach is used to identify communities of nodes in a graph based on the edges connecting them. The method is flexible and allows us to cluster non graph data as well. Spectral Clustering method is a growing clustering algorithm which has performed better than many traditional clustering algorithms in many cases.  \n",
    "Before we start the tutorial, let's look at an example where we *don't* need spectral clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do some standard import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "np.random.seed(1111)\n",
    "X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)\n",
    "plt.scatter(X[:,0], X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create this example, I first ran the code in a Jupyter Notebook, and added the line `plt.savefig(\"Blog-Post-4-Plot1.png\")` to save the result. I then moved the file `Blog-Post-4-Plot1.png` to the images/ directory of my blog. Finally, I added the line:\n",
    "\n",
    "```\n",
    "![Blog-Post-4-Plot1.png]({{ site.baseurl }}/images/Blog-Post-4-Plot1.png) \n",
    "```\n",
    "immediately beneath the code block. For the rest of the plots, we will add them with the same steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot1.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot1.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the `sklearn.datasets` package and `matplotlib.pyplot` package, we have successfully visualized our data in a scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main task is to separate this data set into two natural \"blobs.\" In this case with our unlabelled data, we already have two natural blobs. To achieve our task, I will implement the K-means algorithm, which has good performance on circular-ish blobs. Before we get started, let's import `KMeans`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters = 2)\n",
    "km.fit(X)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c = km.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot2.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have now successfully grouped our data into clusters using K-Means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harder Clustering\n",
    "\n",
    "That was all well and good, but what if our data is \"shaped weird\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "n = 200\n",
    "X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)\n",
    "plt.scatter(X[:,0], X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot3.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can still make out two meaningful clusters in the data, but now from the plot they aren't blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. Now we implement k-means algorithm again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters = 2)\n",
    "km.fit(X)\n",
    "plt.scatter(X[:,0], X[:,1], c = km.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot4.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed that K-Means is not working very well. That is because it is designed for circular or blob-like clusters and hence won’t work as well with this cresent-shaped data. Therefore, in the following problems, I will derive and implement spectral clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "Construct the *similarity matrix* $\\mathbf{A}$. $\\mathbf{A}$ should be a matrix (2d `np.ndarray`) with shape `(n, n)` (recall that `n` is the number of data points). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When constructing the similarity matrix, use a parameter `epsilon`. For this part, we start with $\\epsilon$ = 0.4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we construct the *similarity matrix* $\\mathbf{A}$ following these rules:  \n",
    "(Let `X[i]` represents the coordinates of data point `i`, and `A[i, j]` represents the entry in the i-th row and j-th column of $\\mathbf{A}$)  \n",
    "- `A[i,j]` = 1 if d(X[i], X[j]) < $\\epsilon$\n",
    "- `A[i,j]` = 1 if d(X[i], X[j]) $\\geq$ $\\epsilon$\n",
    "- `A[i,i]` = 0 The diagonal entries should all be equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid constructing this using for-loops, we will use `pairwise_distances` from sklearn. This function that computes all the pairwise distances and collects them into an appropriate matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "# Function computes all the pairwise distances and collect them into an matrix\n",
    "A = pairwise_distances(X, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the matrix we got. By checking the shape, it is a square nxn matrix. In our case, n = 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.27292462, 1.33315598, ..., 1.9812102 , 1.68337039,\n",
       "        1.94073324],\n",
       "       [1.27292462, 0.        , 1.46325112, ..., 1.93729167, 1.68543003,\n",
       "        1.91287315],\n",
       "       [1.33315598, 1.46325112, 0.        , ..., 0.64857172, 0.35035968,\n",
       "        0.60860868],\n",
       "       ...,\n",
       "       [1.9812102 , 1.93729167, 0.64857172, ..., 0.        , 0.30070415,\n",
       "        0.04219636],\n",
       "       [1.68337039, 1.68543003, 0.35035968, ..., 0.30070415, 0.        ,\n",
       "        0.26255757],\n",
       "       [1.94073324, 1.91287315, 0.60860868, ..., 0.04219636, 0.26255757,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed that the diagonal entries are already 0. Then, let's transform the other distances into 0 and 1's as we want. We could easily do it by implementing this one-line code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 1],\n",
       "       [0, 0, 1, ..., 1, 0, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If dis < epsilon, A[i, j] = 1. otherwise A[i, j] = 0\n",
    "A = np.where((A < epsilon), 1, 0)\n",
    "# Since 0 < epsilon, all the diagonal entries becomes 1\n",
    "# We will change the diagonal entries back to 0\n",
    "np.fill_diagonal(A, 0)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix `A` now contains information about which points are near (within distance $\\epsilon$) which other points. We have successfully constructed a *similarity matrix*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "We now pose the task of clustering the data points in `X` as the task of partitioning the rows and columns of `A`. \n",
    "\n",
    "Let $d_i = \\sum_{j = 1}^n a_{ij}$ be the $i$th row-sum of $\\mathbf{A}$, which is also called the *degree* of $i$. Let $C_0$ and $C_1$ be two clusters of the data points. We assume that every data point is in either $C_0$ or $C_1$. The cluster membership as being specified by `y`. We think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row $i$ of $\\mathbf{A}$) is an element of cluster $C_1$.  \n",
    "\n",
    "The *binary norm cut objective* of a matrix $\\mathbf{A}$ is the function \n",
    "\n",
    "$$N_{\\mathbf{A}}(C_0, C_1)\\equiv \\mathbf{cut}(C_0, C_1)\\left(\\frac{1}{\\mathbf{vol}(C_0)} + \\frac{1}{\\mathbf{vol}(C_1)}\\right)\\;.$$\n",
    "\n",
    "When $N_{\\mathbf{A}}(C_0, C_1)$ is small, a pair of clusters $C_0$ and $C_1$ is considered to be a \"good\" partition of the data. To see why, let's look at the cut term and the volume term separately. \n",
    "\n",
    "\n",
    "#### B.1 The Cut Term\n",
    "\n",
    "$\\mathbf{cut}(C_0, C_1) \\equiv \\sum_{i \\in C_0, j \\in C_1} a_{ij}$ is the *cut* of the clusters $C_0$ and $C_1$.  \n",
    "\n",
    "First, the cut term $\\mathbf{cut}(C_0, C_1)$ is the number of nonzero entries in $\\mathbf{A}$ that relate points in cluster $C_0$ to points in cluster $C_1$. This term is small means that points in $C_0$ shouldn't usually be very close to points in $C_1$. \n",
    "\n",
    "Next, we are going to write a function called `cut(A,y)` to compute the cut term. To avoid using for-loop, I have used list comprehension to construct a list of filtered `A[i, j]` and calculated the sum of the resulted list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(A, y):\n",
    "    \"\"\"\n",
    "    Purpose: Compute the cut term\n",
    "    Input: A: The similarity matrix, 2d np.ndarray\n",
    "           y: The labels of the data,  1d np.ndarray\n",
    "    output: an integer which is the cut term of the given matrix A\n",
    "    \"\"\"\n",
    "    sum = np.sum([A[i,j] for i in np.where(y == 0)[0] for j in np.where(y == 1)[0]])\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our result for the true clusters `y` using the similarity matrix calculated in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut(A, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will generate a random vector of random labels of length `n`, with each label equal to either 0 or 1, and check the cut objective for the random labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1146"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random labels\n",
    "random_label = np.random.randint(0, 2, n)\n",
    "\n",
    "# Check the cut objective\n",
    "cut(A, random_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed that the cut objective for the true labels is *much* smaller than the cut objective for the random labels. This indicates that this part of the cut objective indeed favors the true clusters over the random ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.2 The Volume Term \n",
    "\n",
    "$\\mathbf{vol}(C_0) \\equiv \\sum_{i \\in C_0}d_i$, where $d_i = \\sum_{j = 1}^n a_{ij}$ is the *degree* of row $i$ (the total number of all other rows related to row $i$ through $A$). The *volume* of cluster $C_0$ is a measure of the size of the cluster. \n",
    "\n",
    "If we choose cluster $C_0$ to be small, then $\\mathbf{vol}(C_0)$ will be small and $\\frac{1}{\\mathbf{vol}(C_0)}$ will be large, leading to an undesirable higher objective value. \n",
    "\n",
    "Synthesizing, the binary normcut objective asks us to find clusters $C_0$ and $C_1$ such that:\n",
    "\n",
    "1. There are relatively few entries of $\\mathbf{A}$ that join $C_0$ and $C_1$. \n",
    "2. Neither $C_0$ and $C_1$ are too small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to write a function called `vols(A,y)` which computes the volumes of $C_0$ and $C_1$, returning them as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vols(A, y):\n",
    "    \"\"\"\n",
    "    Purpose: Compute the volumn term\n",
    "    Input: A: The similarity matrix, 2d np.ndarray\n",
    "           y: The labels of the data,  1d np.ndarray\n",
    "\n",
    "    Output: A tuple which contains the volumes of cluster C_0 and C_1\n",
    "    \"\"\"\n",
    "    # volume of cluster C_0\n",
    "    vol_c0 = np.sum(A[y== 0])\n",
    "    # volume of cluster C_1\n",
    "    vol_c1 = np.sum(A[y==1])\n",
    "\n",
    "    return (vol_c0, vol_c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the binary normalized cut objective of similarity matrix with the clustering vector, we will write a function called `normcut(A, y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normcut(A, y):\n",
    "    \"\"\"\n",
    "    Purpose: Compute the binary normalized cut objective\n",
    "    Input: A: The similarity matrix, 2d np.ndarray\n",
    "           y: The labels of the data,  1d np.ndarray\n",
    "\n",
    "    Output: A float which is the binary normalized cut objective of A and y\n",
    "    \"\"\"\n",
    "    # computing the volumes\n",
    "    vol = vols(A, y)\n",
    "    # computing the norm cut\n",
    "    norm = cut(A, y)*((1/vol[0])+(1/vol[1]))\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compare the `normcut` objective using both the true labels `y` and the fake labels we generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011518412331615225"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normcut for true labels\n",
    "normcut(A, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0174722557753284"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normcut for random labels\n",
    "normcut(A, random_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed that the `normcut` objective for the true labels is much smaller than the `normcut` objective for the random labels. This makes sense, as we would expect less error in non-trivially generated values with our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! We have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $A$ and (b) not too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, it may not be possible to find the best clustering in practical time, even for relatively small data sets. We will need to use a math trick to solve this!\n",
    "\n",
    "We define a new vector $\\mathbf{z} \\in \\mathbb{R}^n$ such that: \n",
    "\n",
    "$$\n",
    "z_i = \n",
    "\\begin{cases}\n",
    "    \\frac{1}{\\mathbf{vol}(C_0)} &\\quad \\text{if } y_i = 0 \\\\ \n",
    "    -\\frac{1}{\\mathbf{vol}(C_1)} &\\quad \\text{if } y_i = 1 \\\\ \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Note that the signs of  the elements of $\\mathbf{z}$ contain all the information from $\\mathbf{y}$: if $i$ is in cluster $C_0$, then $y_i = 0$ and $z_i > 0$.  \n",
    "Let’s write a function called transform(A, y) to construct the vector $\\mathbf{z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(A, y):\n",
    "    \"\"\"\n",
    "    Purpose: Compute the z vector\n",
    "    Input: A: The similarity matrix, 2d np.ndarray\n",
    "           y: The labels of the data,  1d np.ndarray\n",
    "    Output: A np.ndarray\n",
    "    \"\"\"\n",
    "    # extracting the volumes of clusters\n",
    "    vol = vols(A, y)\n",
    "    # computing the n-dimensional vector z\n",
    "    z = np.where(y == 0, 1/vol[0], -1/vol[1])\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the above function to show that \n",
    "\n",
    "$$\\mathbf{N}_{\\mathbf{A}}(C_0, C_1) = \\frac{\\mathbf{z}^T (\\mathbf{D} - \\mathbf{A})\\mathbf{z}}{\\mathbf{z}^T\\mathbf{D}\\mathbf{z}}\\;,$$\n",
    "\n",
    "where $\\mathbf{D}$ is the diagonal matrix with nonzero entries $d_{ii} = d_i$, and  where $d_i = \\sum_{j = 1}^n a_i$ is the degree (row-sum) from before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011518412331615225"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First compute the life hand side\n",
    "LHS = normcut(A, y)\n",
    "LHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011518412331615088"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the right hand side\n",
    "\n",
    "# Get the z vector\n",
    "z = transform(A, y)\n",
    "# diagonal matrix\n",
    "D = np.diag(np.sum(A, axis=0))\n",
    "\n",
    "RHS = (z @ (D - A) @ z)/(z @ D @ z)\n",
    "RHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the equation above is exact, but computer arithmetic is not, we will use `np.isclose()` to check if left hand side and right hand side are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if they are close\n",
    "np.isclose(LHS, RHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have verified the above equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the identity $\\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0$, where $\\mathbb{1}$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $\\mathbf{z}$ should contain roughly as many positive as negative entries. Similarly, we will also use`np.isclose()` to check if left hand side and right hand side are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if identity is close to 0\n",
    "np.isclose(z.T @ D @ np.ones(n), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have verified that the identity $\\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D\n",
    "\n",
    "In the part C, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function \n",
    "\n",
    "$R_\\mathbf{A}(\\mathbf{z})\\equiv \\frac{\\mathbf{z}^T (\\mathbf{D} - \\mathbf{A})\\mathbf{z}}{\\mathbf{z}^T\\mathbf{D}\\mathbf{z}}$ subject to the condition $\\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0$. \n",
    "\n",
    "Turns out it's actually possible to incorporate this condition into the optimization, by substituting for $\\mathbf{z}$ the orthogonal complement of $\\mathbf{z}$ relative to $\\mathbf{D}\\mathbf{1}$. In the code below, the `orth_obj` function will handle this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orth(u, v):\n",
    "    return (u @ v) / (v @ v) * v\n",
    "\n",
    "e = np.ones(n) \n",
    "\n",
    "d = D @ e\n",
    "\n",
    "def orth_obj(z):\n",
    "    z_o = z - orth(z, d)\n",
    "    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $\\mathbf{z}$. Note that this computation might take a little while since explicit optimization can be pretty slow. We name the minimizing vector `z_min`. \n",
    "\n",
    "We will use the *continuous relaxation* of the normcut problem. We have originally specified that the entries of $\\mathbf{z}$ should take only one of two values (back in Part C), whereas now we're allowing the entries to have *any* value. This means that we are no longer exactly optimizing the normcut objective, but rather an approximation.\n",
    "\n",
    "The minimize function requires an “initial guess” for what we think our minimum is near, so we’ll provide it a vector of ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.57168397,  2.65952192, -0.25557377, -0.11016068, -0.51454006,\n",
       "       -0.31378334, -0.5681382 , -0.41030021,  2.08499242,  1.71165096,\n",
       "        2.34574041, -0.29933846,  0.50699752,  2.30473925,  2.86879528,\n",
       "        2.5689054 ,  2.12020462, -0.21253969, -0.41595232, -0.39363013,\n",
       "        2.35417728,  1.18536707,  2.8198377 , -0.11508422, -0.71512535,\n",
       "        1.88323831, -0.28756431,  2.16476716,  2.17242051, -0.25938271,\n",
       "       -0.32823862,  2.6965414 ,  2.57868177,  2.41009744,  2.13811709,\n",
       "        2.65994967, -0.71512535,  2.93108104,  2.73666293, -0.3509846 ,\n",
       "        2.44523814, -0.21382501, -0.30296259, -1.02668978, -0.66590363,\n",
       "       -0.12820942, -0.2132045 ,  2.40439311,  2.77275035,  0.87722474,\n",
       "       -0.28403835, -0.24624875,  2.8198377 ,  2.28116952, -0.80542876,\n",
       "       -0.49250765,  2.70916014,  1.42613162,  2.80812761, -0.16716665,\n",
       "       -0.98215657, -0.47073932,  2.0333259 , -0.24653334,  2.13780167,\n",
       "       -0.41077021, -0.98215656, -0.71119048, -0.11306391,  2.86016229,\n",
       "        2.52776626,  2.86849087,  2.77957699, -0.17101859, -0.53037425,\n",
       "        0.08591642,  1.86262459, -0.21295121,  2.89174522, -0.38900489,\n",
       "       -0.04622392, -0.48647262, -0.46824474,  2.41054319,  2.39634604,\n",
       "        2.72448162,  2.41009743, -0.14251881, -0.29172233, -0.21712233,\n",
       "       -0.49127374,  2.03332589, -0.32519145, -0.28403836,  1.42613163,\n",
       "        1.71165098,  2.93108105,  2.76091886, -0.59299165, -0.24624876,\n",
       "        2.93702834,  2.77275036,  2.67293406, -0.25657244,  2.46181643,\n",
       "        2.46070837, -0.57778651, -0.16301965,  2.63911193,  2.29002438,\n",
       "       -0.52487035, -0.43911186, -0.24668322,  2.5830574 , -0.29950787,\n",
       "       -0.76030331, -1.29607183, -0.5681382 ,  1.42613162,  2.59209267,\n",
       "        3.00377776, -0.36660551,  2.17248425,  2.79095052,  2.73824715,\n",
       "       -0.50360192,  2.69553051, -0.12594993,  2.6965414 ,  1.42613161,\n",
       "       -0.60732001, -0.5681382 , -0.16716665, -0.30575525,  2.86115267,\n",
       "        2.94250418,  2.42186905,  2.2114344 ,  2.31023328,  2.40597939,\n",
       "        1.77233399,  2.83722653, -0.48131941, -0.20153126,  2.44550647,\n",
       "       -0.48495545, -0.51887822, -0.43188903, -0.3024077 , -0.23877974,\n",
       "       -0.30296258, -0.60732002, -0.53037424,  1.88323832,  2.88684628,\n",
       "       -0.98215657,  2.65459351, -0.24624875, -0.48495544, -0.46824475,\n",
       "        2.67275289,  2.63684493, -0.40131985, -0.1425188 ,  1.48499536,\n",
       "        1.88323831,  2.46070837, -0.20153128, -0.5681382 , -0.64672318,\n",
       "        2.95933067, -0.47976947,  2.29002437,  2.41889687,  2.72448161,\n",
       "       -0.25673601, -0.25663543, -0.43188901, -0.12040114,  2.79095053,\n",
       "        2.12020461, -0.50934658, -0.47073933,  1.87949834,  2.08499243,\n",
       "        2.01341011, -0.38607332,  2.13811708,  2.70916013,  2.77275035,\n",
       "       -0.25673602,  2.67275289, -0.38607334,  2.93039261,  2.4386269 ,\n",
       "        2.5689054 ,  1.98910753, -0.19181653, -0.04767512, -0.18756374])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "# extracting the minimum vector\n",
    "z_min = optimize.minimize(orth_obj, np.ones(len(z))).x\n",
    "\n",
    "z_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E\n",
    "\n",
    "Since by design, only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. Let's plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if z_min[i] < 0, label 1. otherwise label 0\n",
    "labels = np.where(z_min < 0, 1, 0) \n",
    "\n",
    "# Plotting the original data\n",
    "plt.scatter(X[:,0], X[:,1], c = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot5.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with the true label plot before Part A, there is a little discrepancy, but overall it is doing pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part F\n",
    "\n",
    "Explicitly optimizing the orthogonal objective is  *way* too slow to be practical. If spectral clustering required that we do this each time, no one would use it. \n",
    "\n",
    "The Laplacian matrix is another method we can use to put together our spectral clustering algorithm. Essentially, what we are doing is finding the eigenvalues and eigenvector that will generate our labels. \n",
    "\n",
    "The Rayleigh-Ritz Theorem states that the minimizing $\\mathbf{z}$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem \n",
    "\n",
    "$$ (\\mathbf{D} - \\mathbf{A}) \\mathbf{z} = \\lambda \\mathbf{D}\\mathbf{z}\\;, \\quad \\mathbf{z}^T\\mathbf{D}\\mathbb{1} = 0$$\n",
    "\n",
    "which is equivalent to the standard eigenvalue problem \n",
    "\n",
    "$$ \\mathbf{D}^{-1}(\\mathbf{D} - \\mathbf{A}) \\mathbf{z} = \\lambda \\mathbf{z}\\;, \\quad \\mathbf{z}^T\\mathbb{1} = 0\\;.$$\n",
    "\n",
    "Well, $\\mathbb{1}$ is actually the eigenvector with smallest eigenvalue of the matrix $\\mathbf{D}^{-1}(\\mathbf{D} - \\mathbf{A})$. So, the vector $\\mathbf{z}$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct the matrix $\\mathbf{L} = \\mathbf{D}^{-1}(\\mathbf{D} - \\mathbf{A})$, which is often called the (normalized) *Laplacian* matrix of the similarity matrix $\\mathbf{A}$. Find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the normalized Laplacian matrix of the similarity matrix A\n",
    "L = np.linalg.inv(D) @ (D-A) \n",
    "\n",
    "# Extracting the eigenvalues and eigenvectors of L\n",
    "eigenValues, eigenVectors = np.linalg.eig(L)\n",
    "\n",
    "idx = eigenValues.argsort()   \n",
    "sorted_values = eigenValues[idx]\n",
    "sorted_vectors = eigenVectors[:,idx]\n",
    "# vector corresponding to 2nd smallest eigenvector\n",
    "z_eig = sorted_vectors[:, 1]\n",
    "label= np.where(z_eig < 0, \"blue\", \"red\")\n",
    "plt.scatter(X[:,0], X[:,1], c = label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot6.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resulting plot is very similar to the plot with original label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part G\n",
    "\n",
    "Now, let's put together everything we have done in the previous parts. Let's write a function called `spectral_clustering(X, epsilon)` which performs spectral clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_clustering(X, epsilon):\n",
    "    \"\"\"\n",
    "    Purpose: Performs spectral clustering\n",
    "    Input: X : The array of input data. np.ndarray\n",
    "           epsilon : The distance threshold. float\n",
    "    Output: Returns an array of binary labels indicating whether data point i is in group 0 or group 1\n",
    "    \"\"\"\n",
    "    # Construct the similarity matrix A\n",
    "    A = pairwise_distances(X, X) \n",
    "    A = np.where((A < epsilon), 1, 0)\n",
    "    np.fill_diagonal(A, 0)\n",
    "    \n",
    "    # Construct the Laplacian matrix L\n",
    "    D = np.diag(A.sum(axis=0)) # construct diagonal matrix\n",
    "    L = np.linalg.inv(D) @ (D - A)\n",
    "    \n",
    "    # Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix\n",
    "    evals, evecs = np.linalg.eig(L)\n",
    "    sorted_vecs = evecs[:, np.argsort(evals)]\n",
    "    z_eig = sorted_vecs[:, 1]\n",
    "    \n",
    "    # Return labels based on z_eig\n",
    "    labels = np.where(z_eig < 0, 1, 0) \n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the new labels\n",
    "label = spectral_clustering(X, 0.4)\n",
    "# Plotting the data\n",
    "plt.scatter(X[:,0], X[:,1], c = label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot7.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot we generate is the same as Part F. Therefore our function is able to perform the spectral clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part H\n",
    "\n",
    "Next, we are going to run a few experiments using our function, by generating different data sets using `make_moons`. We will see what happens when we increase the `noise`. For all the experiments, we will use `n = 1000`. Again, we will still use $\\epsilon$ = 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment 1: noise = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(555)\n",
    "# Generate the data set\n",
    "X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)\n",
    "\n",
    "# Perform spctral cluster\n",
    "label = spectral_clustering(X, epsilon)\n",
    "plt.scatter(X[:,0], X[:,1], c=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot11.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral clustering has successfully found the two half-moon clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: noise = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "np.random.seed(555)\n",
    "# Generate the data set\n",
    "X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)\n",
    "\n",
    "# Perform spctral cluster\n",
    "label = spectral_clustering(X, epsilon)\n",
    "plt.scatter(X[:,0], X[:,1], c=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot8.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral clustering has successfully found the two half-moon clusters with only few misclassified points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: noise = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(555)\n",
    "# Generate the data set\n",
    "X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.15, random_state=None)\n",
    "\n",
    "# Perform spctral cluster\n",
    "label = spectral_clustering(X, epsilon)\n",
    "plt.scatter(X[:,0], X[:,1], c=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot9.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral clustering has successfully found the two half-moon clusters, however there are quite a lot of misclassified points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: noise = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(555)\n",
    "# Generate the data set\n",
    "X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.2, random_state=None)\n",
    "\n",
    "# Perform spctral cluster\n",
    "label = spectral_clustering(X, epsilon)\n",
    "plt.scatter(X[:,0], X[:,1], c=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot10.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral clustering can hardly found the two half-moon clusters. A lot of points have been misclassified. It is even worse than Experiment 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From all our experiments, we can conclude as noise increases, the spectral clustering algorithm still works, but it starts to make lots of mistakes. I would not sugggest use spectral clustering when noise is large.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I\n",
    "\n",
    "Now let's try our spectral clustering function on another data set -- the bull's eye! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)\n",
    "plt.scatter(X[:,0], X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot20.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two concentric circles. As before k-means will not do well here at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters = 2)\n",
    "km.fit(X)\n",
    "plt.scatter(X[:,0], X[:,1], c = km.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot12.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try spectral clustering to see if it can successfully separate the two circles. We will test with a series of $\\epsilon$ to get a range of $\\epsilon$ that can correctly separate the two circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: $\\epsilon$ = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.2\n",
    "n = 1000\n",
    "# Generate the dataset\n",
    "X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)\n",
    "# Perform spctral cluster\n",
    "label = spectral_clustering(X, epsilon)\n",
    "plt.scatter(X[:,0], X[:,1], c=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot13.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\epsilon$ = 0.2, the spectral clustering algorithm cannot separate the two circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment 2: $\\epsilon$ = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.25\n",
    "n = 1000\n",
    "# Generate the dataset\n",
    "X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)\n",
    "# Perform spctral cluster\n",
    "label = spectral_clustering(X, epsilon)\n",
    "plt.scatter(X[:,0], X[:,1], c=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot14.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\epsilon$= 0.25, the spectral clustering algorithm can successfully separate the two circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: $\\epsilon$ = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.4\n",
    "n = 1000\n",
    "# Generate the dataset\n",
    "X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)\n",
    "# Perform spctral cluster\n",
    "label = spectral_clustering(X, epsilon)\n",
    "plt.scatter(X[:,0], X[:,1], c=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot15.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\epsilon$= 0.4, the spectral clustering algorithm still can successfully separate the two circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: $\\epsilon$ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.5\n",
    "n = 1000\n",
    "# Generate the dataset\n",
    "X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)\n",
    "# Perform spctral cluster\n",
    "label = spectral_clustering(X, epsilon)\n",
    "plt.scatter(X[:,0], X[:,1], c=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot16.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\epsilon$= 0.5, the spectral clustering algorithm still can successfully separate the two circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5: $\\epsilon$ = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.6\n",
    "n = 1000\n",
    "# Generate the dataset\n",
    "X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)\n",
    "# Perform spctral cluster\n",
    "label = spectral_clustering(X, epsilon)\n",
    "plt.scatter(X[:,0], X[:,1], c=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot17.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\epsilon$= 0.6, the spectral clustering algorithm cannot separate the two circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6: $\\epsilon$ = 0.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.52\n",
    "n = 1000\n",
    "# Generate the dataset\n",
    "X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)\n",
    "# Perform spctral cluster\n",
    "label = spectral_clustering(X, epsilon)\n",
    "plt.scatter(X[:,0], X[:,1], c=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot18.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\epsilon$= 0.52, the spectral clustering algorithm still can successfully separate the two circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7: $\\epsilon$ = 0.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.53\n",
    "n = 1000\n",
    "# Generate the dataset\n",
    "X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)\n",
    "# Perform spctral cluster\n",
    "label = spectral_clustering(X, epsilon)\n",
    "plt.scatter(X[:,0], X[:,1], c=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blog-Post-4-Plot19.png](https://raw.githubusercontent.com/JadenWSR/JadenWSR.github.io/master/images/Blog-Post-4-Plot19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\epsilon$= 0.53, the spectral clustering algorithm cannot separate the two circles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After all the experiments, I have found that when 0.25$\\leq \\epsilon \\leq$0.52, the spectral clustering algorithm can successfully separate the two circles.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
